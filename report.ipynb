{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Link to github repo: https://github.com/Edyarich/dlsystems-final-project"
      ],
      "metadata": {
        "id": "439joBGq2pvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) Math part** \n",
        "To understand how to code this kind of model, first we need to explore math intuition behind. This paperы were used to understand all the background: \\\\\n",
        "1) https://arxiv.org/abs/2105.05233 \\\\\n",
        "2) https://arxiv.org/abs/2102.09672 \\\\\n",
        "All in all we have such a **pipeline**: \\\\\n",
        "Model has two passes - forward and backward. **Forward process** is following:\n",
        "\n",
        "> We need to make the pure noise from our image, by adding some noise from a Gaussian distrubution several times. The mean and variance are parameters, which we need to generate some noise. Actually we will change only mean, so let us take the variance out of speech (**PROBABLY SHOULD CHANGE THIS SENTENCE**). The way how we change our mean, depending on the timestep is called a **schedule**. We have implemented two schedules: linear and cosine, but there are a lot more variants to generate means for our noises. Linear schedule just means that we have beggining and ending numbers for the mean value and we make a linear function with f(0) = beggining, f(T) = ending; where T - number of timesteps. \\\\\n",
        "$q(x_t|x_{t-1}) := \\mathcal{N} (x_t;\\sqrt{1-β_t}x_{t-1}, β_t𝐈)$ \\\\\n",
        "In code we will not apply this forward function *n* times to get *n_noised* image. There is a way, how we can recalculate the mean from schedule and jump from input image to the image at step *n*. More formulas and proves are presented in the following papers and the result are used in code, in __init__ function for Diffusion model class. \\\\\n",
        "Denote $α_t := 1 - β_t$ and $α_{t}^{'} := ∏_{s=0}^t α_s$ \\\\\n",
        "Then forward process will look like: \\\\\n",
        "$q(x_t|x_0) = \\mathcal{N} (x_t; \\sqrt{α_{t}^{'}}x_0, (1-α_{t}^{'})𝐈)$ \\\\\n",
        "Also forward process function is denoted by **q** and in code we follow this mark, so all q_sample function or etc. are about forward process"
      ],
      "metadata": {
        "id": "5ZRK3PFgVIjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us talk about the most interesting part - **Backward process**:\n",
        "> In backward process (denoted by **p**, same logic with sentence above) we want to predict image before noising, when the given input is noised image. Since forward process is about adding the noise tensors to our input tensor, we can simplify the goal to predict the noise, which was added at *ith* step. Authors of DDPM model itself and authors of papers provided us with results and pipeline of the experience, which they made about how exactly we can predict the noise. Final decision and answer is just to predict the mean to generate the noise same as in forward process. \\\\\n",
        "At this part we need to choose the deep learning model, which will be used to do this prediction. Our decision is to use UNet architecture. We built it from scratch, also building new operations and modules, but more about this in further proposal. \\\\\n",
        "In conslusion, we will learn our model to predict the noise from input image and given timestep and backward pass will be done. Now, to generate new sample we will give our model some pure noise and it will be recovering the image from it, as practice shows, we will get some brand new images. \\\\\n",
        "$p_θ(x_{t-1}|x_t) := \\mathcal{N}(x_{t-1}; μ_{θ}(x_t, t), ∑_{θ}(x_t, t))$, where $θ$ we try to learn"
      ],
      "metadata": {
        "id": "EmkXksBtVSvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Diffusion class** \n",
        "In **apps/diffusion.py** we started with implementing the Diffusion class. It is a main block of our new model. It also requires some helper functions: \n",
        "> **2.1) extract** makes the tensor, given the betas array and timesteps subarray \\\\\n",
        "```python\n",
        "  def extract(x, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    device = x.device\n",
        "    out_handle = device.empty((batch_size,))\n",
        "    for i in range(batch_size):\n",
        "        ind = int(t.numpy()[i])\n",
        "        out_handle[i] = x.cached_data[ind]\n",
        "    new_shape = (batch_size,) + (1,) * (len(x_shape) - 1)\n",
        "    return Tensor(out_handle, device=device).reshape(new_shape)\n",
        "```\n",
        "**2.2) schedules** what is time schedule we discussed in math block, so there are implementations of two types of them: linear and cosine\n",
        "```python\n",
        "  def linear_beta_schedule(timesteps, device=None):\n",
        "      scale = 1000 / timesteps\n",
        "      beta_start = scale * 0.0001\n",
        "      beta_end = scale * 0.02\n",
        "      return Tensor(array_api.linspace(beta_start, beta_end, timesteps),\n",
        "                    dtype=\"float32\", device=device)\n",
        "```\n",
        ">\n",
        " At the initialization moment we setup all the preferences: \n",
        " > **a) noise schedule** - the rule, how noise will be added at each forward step. there are two options: linear and cosine schedule \\\\\n",
        "**b) denoising model** - model, which will be used to predict the noise and make the backward process work, more about this part a bit later \\\\\n",
        "**c) timesteps** - just a number of noising steps \\\\\n",
        "**d)** **loss** function and **device** where model exists. \n",
        ">\n",
        "Our class methods are: \\\\\n",
        "\n",
        ">**2.1) q_sample** function applies forward noising process. Uses extract [2.1] function and some pre-calculated data (more in math block) from betas, which were given by time schedule. \\\\\n",
        "```python\n",
        "  def q_sample(self, x_0, t, noise=None):\n",
        "        shape = x_0.shape\n",
        "        noise = x_0.device.randn(*shape) if noise is None else noise\n",
        "        return (\n",
        "            (extract(self.sqrt_alphas_cumprod, t, x_0.shape).broadcast_to(shape) * x_0 +\n",
        "             extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape).broadcast_to(shape) * noise).data\n",
        "        )\n",
        "```\n",
        "**2.2) p_sample** - function to apply single step of denoising process. It uses given model to predict the noise on exact step. Also we do some betas (from schedule usage) transformations, such as making tensor from them, then generating cocoefficients to jump right into the ith step. \\\\\n",
        "```python\n",
        "  def p_sample(self, model, x, t, t_index):\n",
        "        betas_t = extract(self.betas, t, x.shape).data.broadcast_to(x.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "            self.sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "        ).data.broadcast_to(x.shape)\n",
        "        sqrt_recip_alphas_t = extract(\n",
        "            self.sqrt_recip_alphas, t, x.shape\n",
        "        ).data.broadcast_to(x.shape)\n",
        "        model_mean = (sqrt_recip_alphas_t * (\n",
        "                x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "        )).data\n",
        "        if t_index == 0:\n",
        "            return model_mean\n",
        "        else:\n",
        "            posterior_variance_t = extract(\n",
        "                self.posterior_variance, t, x.shape\n",
        "            ).data.broadcast_to(x.shape)\n",
        "            noise = init.randn(*x.shape, device=x.device, requires_grad=False)\n",
        "            return model_mean + ops.sqrt(posterior_variance_t) * noise\n",
        "```\n",
        "**2.3) sample_loop** - function, which applies all the denoising steps, just returning the result of **p_sample_loop** with following logic: \\\\\n",
        "**a)** generation of pure noise image with help of init.randn function \\\\\n",
        "**b)** apply **p_sample** function to an previous image (image always changing, during applying this functions) with given model\n",
        "**c)** return the array, where we have stored all the steps of denoising an image\n",
        " \\\\\n",
        "```python\n",
        "  def p_sample_loop(self, shape):\n",
        "        model = self.denoise_model\n",
        "        device = model.parameters()[0].device\n",
        "        batch_size = shape[0]\n",
        "        img = init.randn(*shape, device=device, requires_grad=False)\n",
        "        imgs = []\n",
        "        for i in tqdm(reversed(range(0, self.timesteps)),\n",
        "                      desc='sampling loop time step', total=self.timesteps):\n",
        "            img = self.p_sample(\n",
        "                model,\n",
        "                img,\n",
        "                init.constant(\n",
        "                    batch_size,\n",
        "                    c=i,\n",
        "                    device=device,\n",
        "                    requires_grad=False\n",
        "                ),\n",
        "                t_index=i\n",
        "            )\n",
        "            imgs.append(img.detach().numpy())\n",
        "        return imgs\n",
        "```\n",
        ">"
      ],
      "metadata": {
        "id": "9rEcHlsmVTOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3) UNetBlock class** \n",
        "Really massive work has been done on this predictioning model and UnetBlock is a core part of the Unet class, which will be described a bit later. First, let us talk about operations, which were added to `needle/ops.py`: **ConvTranspose** and **MaxPool** \\\\\n",
        "\n",
        "Block contains such layers: \n",
        "> **a)** Time embedding with nn.Linear layer and then nn.ReLU in forward function \\\\\n",
        " **b)** Convolution layer with arguments, depend on which UNet part we use this block (up or down) \\\\\n",
        "**c)** Second convolution \\\\\n",
        "**d)** Batch normalization over the result of the successively used layers on the input \\\\\n",
        "> **e)** Returned data is a transformation, applied to result of **d)** step. If we are upsampling - this transformation is ConvTranspose, if we downsampling it will be MaxPool\n",
        "```python\n",
        "  class UnetBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False, device=None):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch, device=device)\n",
        "        if up:\n",
        "            self.conv1 = nn.Conv(2 * in_ch, out_ch, 3, padding=1, device=device)\n",
        "            self.transform = nn.ConvTranspose(out_ch, out_ch, 4, 2, 1, device=device)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv(in_ch, out_ch, 3, padding=1, device=device)\n",
        "            self.transform = nn.MaxPool(2)\n",
        "        self.conv2 = nn.Conv(out_ch, out_ch, 3, padding=1, device=device)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch, device=device)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch, device=device)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x, t):\n",
        "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        time_emb = self.relu(self.time_mlp(t))\n",
        "        time_emb = time_emb.reshape(time_emb.shape + (1, 1)).broadcast_to(h.shape)\n",
        "        h = h + time_emb\n",
        "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
        "        return self.transform(h)\n",
        "```"
      ],
      "metadata": {
        "id": "aPdfvcNmVTko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1) ConvTranspose**\n",
        "As a matter of fact, ConvTranspose is a combination of two layers: Upsampling and Convolution.  \n",
        "So, our implementation do exactly the same: it modifies the input using `ops.dilate` and then applies convolution.\n",
        "\n",
        "```python\n",
        "class ConvTranspose(Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int,\n",
        "                 stride: int = 1, padding: int = 0, output_padding: int = 0,\n",
        "                 bias: bool = True, device=None, dtype: str = \"float32\"):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_padding = output_padding + kernel_size - 1 - padding\n",
        "        self.conv = Conv(in_channels, out_channels, kernel_size, 1,\n",
        "                         conv_padding, bias, device, dtype, True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        dilated_x = ops.dilate(\n",
        "            x, axes=(2, 3), dilation=self.stride - 1, cut_last=True\n",
        "        )\n",
        "        return self.conv(dilated_x)\n",
        "```"
      ],
      "metadata": {
        "id": "FLfn_Ar4ZP32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2) MaxPool**\n"
      ],
      "metadata": {
        "id": "BzyrjNH2anaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have implemented the basic version of maxpooling with stride = kernel_size, with no padding and dilation.\n",
        "There are two reasons for this decision\n",
        "1. No need for the modified MaxPool layer, the simple one is OK for UNet\n",
        "2. The gradient calculation of Maxpool layer becomes more sophisticated when we take additional parameters into consideration\n",
        "\n",
        "```python\n",
        "class MaxPool(Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # NCHW ==> NHWC ==> NH'W'O ==> NOH'W'\n",
        "        _x = x.permute((0, 2, 3, 1))\n",
        "        output = ops.maxpool(_x, self.kernel_size)\n",
        "        return output.permute((0, 3, 1, 2))\n",
        "```"
      ],
      "metadata": {
        "id": "Vxdc_-_fbQ3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3) State dictionary**\n",
        "State dictionary is a simple dictionary which contains parameters of the model. They can be easily saved, updated and restored, adding a great deal of modularity to needle models  It is a useful tool for model checkpointing during the training process.  \n",
        "To begin with, we implemented two magic methods `__getstate__` and `__setstate__`, which are necessary for correct pickle serialization.\n",
        "Then, we modified `nn.Module`, adding methods for loading and saving state dictionaries.\n",
        "\n",
        "```python\n",
        "class NDArray:\n",
        "    ...\n",
        "    def __getstate__(self):\n",
        "        # Method for correct pickle serialization of NDArray\n",
        "        # https://codefather.tech/blog/python-pickle/\n",
        "        return {'data': self.numpy(), 'device': self.device.name}\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        self.__init__(state['data'], _find_device(state['device']))\n",
        "```"
      ],
      "metadata": {
        "id": "XocSNp8OuFwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4) UNet class**\n",
        "The class takes all required information about the shape of the images, channels count and time encoding. This preferences will be used in the layers and block initializations. All the timesteps first should be encoded with sinusoidal positional embedding, which is implemented in `SinusoidalPosEmb` class. Required functions of sinus and cosinus were added both in backend ndarray and in `needle/ops.py` \\\\\n",
        "Then it builds the complete UNet architecture: \\\\\n",
        "> **a)** Time embedding is actually block of connected layers by `nn.Sequential`: `SinPosEmb`, `nn.Linear` and `nn.ReLU` layers. \\\\\n",
        "**b)** Projection is a simple convolution \\\\\n",
        "**c)** Then we initialize two list for downsampling and upsampling branches and fill this lists with already implemented UNetBlocks with parameters from input arguments \\\\\n",
        "**d)** In forward process the model predicts noise having noisy image and its timestamp\n",
        "> "
      ],
      "metadata": {
        "id": "f9PvnoCyVTxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5) Learning model**\n",
        "The whole pipeline is available in `main.ipynb`  \n",
        "**5.1) Dataset** We chose landscapes dataset from kaggle: https://www.kaggle.com/datasets/arnaud58/landscape-pictures?resource=download \\\\\n",
        "Following **LandscapesDataset class** was added in **needle/data.py** and we instantly apply resize and normalize transformation for more accurate work of the model and faster learning. \\\\\n",
        "**5.2) Learning.ipynb** First cells are required to setup cuda, build the cpp files, clone the code from github and download the landscapes dataset. To train we chose this parameters: optimizer: Adam, loss: L2, timesteps (number of forward and backward steps): 300, 2 epoches and batch consist of 12 images. \\\\\n",
        "**Training process** took us 1 hour with a result l2 loss of 0.324\n",
        "\n"
      ],
      "metadata": {
        "id": "R2ZFYY5pVT93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------"
      ],
      "metadata": {
        "id": "jxAS5GhV25s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BACKUP\n",
        "\n",
        "### Work by Eduard and Michael.\n",
        "Our work on this project consist of several parts, all of them will be described in further text.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **1) Math part** \n",
        "To understand how to code this kind of model, first we need to explore math intuition behind. This paperы were used to understand all the background: \\\\\n",
        "1) https://arxiv.org/abs/2105.05233 \\\\\n",
        "2) https://arxiv.org/abs/2102.09672 \\\\\n",
        "All in all we have such a **pipeline**: \\\\\n",
        "Model has two passes - forward and backward. **Forward process** is following:\n",
        "\n",
        "> We need to make the pure noise from our image, by adding some noise from a Gaussian distrubution several times. The mean and variance are parameters, which we need to generate some noise. Actually we will change only mean, so let us take the variance out of speech (**PROBABLY SHOULD CHANGE THIS SENTENCE**). The way how we change our mean, depending on the timestep is called a **schedule**. We have implemented two schedules: linear and cosine, but there are a lot more variants to generate means for our noises. Linear schedule just means that we have beggining and ending numbers for the mean value and we make a linear function with f(0) = beggining, f(T) = ending; where T - number of timesteps. \\\\\n",
        "In code we will not apply this forward function *n* times to get *n_noised* image. There is a way, how we can recalculate the mean from schedule and jump from input image to the image at step *n*. More formulas and proves are presented in the following papers and the result are used in code, in __init__ function for Diffusion model class. Also forward process function is denoted by **q** and in code we follow this mark, so all q_sample function or etc. are about forward process\n",
        ">\n",
        "Now let us talk about the most interesting part - **Backward process**:\n",
        "> In backward process (denoted by **p**, same logic with sentence above) we want to predict image before noising, when the given input is noised image. Since forward process is about adding the noise tensors to our input tensor, we can simplify the goal to predict the noise, which was added at *ith* step. Authors of DDPM model itself and authors of papers provided us with results and pipeline of the experience, which they made about how exactly we can predict the noise. Final decision and answer is just to predict the mean to generate the noise same as in forward process. \\\\\n",
        "At this part we need to choose the deep learning model, which will be used to do this prediction. Our decision is to use UNet architecture. We built it from scratch, also building new operations and modules, but more about this in further proposal. \\\\\n",
        "In conslusion, we will learn our model to predict the noise from input image and given timestep and backward pass will be done. Now, to generate new sample we will give our model some pure noise and it will be recovering the image from it, as practice shows, we will get some brand new images.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **2) Diffusion class** \n",
        "In **apps/diffusion.py** we started with implementing the Diffusion class. It is a main block of our new model. It also requires some helper functions: \n",
        "> **2.1) extract** TODO \\\\\n",
        "**2.2) schedules** what is time schedule we discussed in math block, so there are implementations of two types of them: linear and cosine\n",
        ">\n",
        " At the initialization moment we setup all the preferences: \n",
        " > **a) noise schedule** - the rule, how noise will be added at each forward step. there are two options: linear and cosine schedule \\\\\n",
        "**b) denoising model** - model, which will be used to predict the noise and make the backward process work, more about this part a bit later \\\\\n",
        "**c) timesteps** - just a number of noising steps \\\\\n",
        "**d)** **loss** function and **device** where model exists. \n",
        ">\n",
        "Our class methods are: \\\\\n",
        "\n",
        ">**2.1) q_sample** function applies forward noising process. Uses extract [2.1] function and some pre-calculated data (more in math block) from betas, which were given by time schedule. \\\\\n",
        "**2.2) p_sample** - function to apply single step of denoising process. It uses given model to predict the noise on exact step. Also we do some betas (from schedule usage) transformations, such as making tensor from them, then generating cocoefficients to jump right into the ith step. \\\\\n",
        "**2.3) sample_loop** - function, which applies all the denoising steps, just returning the result of **p_sample_loop** with following logic: \\\\\n",
        "**a)** generation of pure noise image with help of init.randn function \\\\\n",
        "**b)** apply **p_sample** function to an previous image (image always changing, during applying this functions) with given model\n",
        "**c)** return the array, where we have stored all the steps of denoising an image\n",
        " \\\\\n",
        ">\n",
        "---\n",
        "###**3) UNetBlock class** \n",
        "Really massive work has been done on this predictioning model and UnetBlock is part of all the Unet class, which will be described a bit later. First, let us talk about operations, which were added to **needle/ops.py** \\\\\n",
        "**3.1) ConvTranspose** TODO \\\\\n",
        "**3.2) MaxPool** TODO \\\\\n",
        "Block contains such a layers: \n",
        "> **a)** Time embedding with nn.Linear layer and then nn.ReLU in forward function \\\\\n",
        " **b)** Convolution layer with arguments, depend on which UNet part we use this block (up or down) \\\\\n",
        "**c)** Second convolution \\\\\n",
        "**d)** Batch normalization over the result of the successively used layers on the input \\\\\n",
        "> **e)** Returned data is transformation, applied to result of **d)** step. If we are upsampling - this transformation is ConvTranspose, if we downsampling it will be MaxPool\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "###**4) UNet class**\n",
        "As input this class takes all required parameters about the images shape and channel numbers. Also takes the info, which will be needed in time encoding. This preferences will be used in the layers and block initializations. All the timesteps first should be encoded with sinusoidal positional embedding, which is implemented in **SinusoidalPosEmb** class. Required functions of sinus and cosinus were added both in backend ndarray and in needle/ops.py \\\\\n",
        "Then it builds the complete UNet architecture: \\\\\n",
        "> **a)** Time embedding is actually block of connected layers by nn.Sequential: SinPosEmb, nn.Linear and nn.ReLU layers. \\\\\n",
        "**b)** Projection is a simple convolution \\\\\n",
        "**c)** Then we initialize two list for downsampling and upsampling branches and fill this lists with already implemented UNetBlocks with parameters from input arguments \\\\\n",
        "**d)** In forward process we **TODO**\n",
        "> \n",
        "\n",
        "---\n",
        "###**5) Learning model**\n",
        "**5.1) Dataset** We chose landscapes dataset from kaggle: https://www.kaggle.com/datasets/arnaud58/landscape-pictures?resource=download \\\\\n",
        "Following **LandscapesDataset class** was added in **needle/data.py** and we instantly apply resize and normalize transformation for more accurate work of the model and faster learning. \\\\\n",
        "**5.2) Learning.ipynb** First cells are required to setup cuda, build the cpp files, clone the code from github and download the landscapes dataset. To train we chose this parameters: optimizer: Adam, loss: L2, timesteps (number of forward and backward steps): 300, 4 epoches and batch consist of 4 images. \\\\\n",
        "**Training process** took us **TODO TIME** with a result loss of **TODO LOSS** "
      ],
      "metadata": {
        "id": "Q6q_8DhsYMqY"
      }
    }
  ]
}